{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processes the raw bike trip data to get information on bike dock locations\n",
    "and when the docks were placed in those locations.\n",
    "\n",
    "Desired output columns:\n",
    "\n",
    "id | first | last | name | lat | lon | rides\n",
    "\n",
    "where\n",
    "- id is the station's id\n",
    "- first is the earliest trip date for the station id\n",
    "- last is the latest trip date for the station id (included in case docks are removed)\n",
    "- name is the station's name\n",
    "- lat and lon are the latitude and longitude of the station's location\n",
    "- rides is a count of the number of rides found in the data -- it is used to remove dummy stations in the data.\n",
    "    only stations with more than RIDES_COUNT_THRESHOLD are included in output\n",
    "\n",
    "This script is abstracte to apply to multiple cities.\n",
    "DON'T FORGET: update the 'CITY' variable\n",
    "\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import math\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "# CITY = 'dc'\n",
    "# CITY = 'boston'\n",
    "# CITY = 'nyc'\n",
    "# CITY = 'chicago'\n",
    "CITY = 'philly'\n",
    "print('city', CITY)\n",
    "\n",
    "\n",
    "RIDES_COUNT_THRESHOLD = 100\n",
    "\n",
    "\n",
    "def get_filepath(city):\n",
    "    return '../data/' + city + '-bike/'\n",
    "\n",
    "\n",
    "def transform_date(date):\n",
    "    try:\n",
    "        dt = datetime.strptime(date.split(' ')[0], '%m/%d/%Y')\n",
    "        # this dataset is so frustrating lol\n",
    "    except ValueError:\n",
    "        dt = datetime.strptime(date.split(' ')[0], '%Y-%m-%d')\n",
    "        \n",
    "    return dt.strftime('%Y-%m-%d')\n",
    "\n",
    "def open_zipfile(zipfilename):\n",
    "    # Because someone dropped some gnarly mac osx files into their zips\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    filenames = [f.filename for f in zipfile.infolist()]\n",
    "    # Return the first file that can be opened  - not all of them have .csv suffix\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            df = pd.read_csv(zipfile.open(filename))\n",
    "            return df\n",
    "        except:\n",
    "            print('failed to open filename from zip', zipfilename, ': ', filename)\n",
    "            pass\n",
    "    raise Exception('unable to read a csv from zipfile %s' % zipfilename)\n",
    "\n",
    "def open_zipfile_dc(zipfilename):\n",
    "## DC bike files from 2012 to 2017 have 4 files for each quarter\n",
    "## this generator yields each of those files\n",
    "\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    files = [f.filename for f in zipfile.infolist()]\n",
    "\n",
    "    for filename in files:\n",
    "        if 'MACOSX' in filename:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(zipfile.open(filename))\n",
    "            yield df\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def open_zipfile_chicago(zipfilename):\n",
    "    ## Some chicago bike files need to be unzipped and yielded\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    files = [f.filename for f in zipfile.infolist()]\n",
    "    for filename in files:\n",
    "        if filename[:8] == '__MACOSX' or filename.endswith('txt') or 'Divvy_Stations_2' in filename:\n",
    "            continue\n",
    "        if '/' in filename:\n",
    "            if '/Divvy_Stations' in filename:\n",
    "                continue\n",
    "        try:\n",
    "            print(filename)\n",
    "            df = pd.read_csv(zipfile.open(filename))\n",
    "            yield df\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "def open_zipfile_philly(zipfilename):\n",
    "    ## Specific function to unzip philly bike files\n",
    "    zipfile = ZipFile(zipfilename)\n",
    "    filenames = [f.filename for f in zipfile.infolist()]\n",
    "    # Return the first file that can be opened  - not all of them have .csv suffix\n",
    "    if len(filenames) == 2:\n",
    "        filename = filenames[1]\n",
    "    else:\n",
    "        filename = filenames[0]\n",
    "    try:\n",
    "        df = pd.read_csv(zipfile.open(filename))\n",
    "        return df\n",
    "    except:\n",
    "        print('failed to open filename from zip', zipfilename, ': ', filename)\n",
    "        pass\n",
    "    raise Exception('unable to read a csv from zipfile %s' % zipfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make a dict like \n",
    "{\"id\": {\"name\": \"\", \"lat\": \"\", \"lon\": \"\", \"first\": \"\", \"last\": \"\"}}\n",
    "where there is one entry for each id\n",
    "and where the start time is always the earliest found\n",
    "\n",
    "and then later transform it into a dict like\n",
    "\n",
    "{'id': [id1, id2, id3], 'col_2': ['a', 'b', 'c', 'd']}\n",
    "\n",
    "to then make into a dataframe and save as a CSV\n",
    "\"\"\"\n",
    "\n",
    "# input file column names for indexing data with\n",
    "start_station_id = 'startstationid'\n",
    "start_station_name = 'startstationname'\n",
    "start_station_latitude = 'startstationlatitude'\n",
    "start_station_longitude = 'startstationlongitude'\n",
    "starttime = 'starttime'\n",
    "\n",
    "\n",
    "    \n",
    "# output file column names\n",
    "ID = 'id'\n",
    "NAME = 'name'\n",
    "LAT = 'lat'\n",
    "LON = 'lon'\n",
    "FIRST = 'first'\n",
    "LAST = 'last'\n",
    "RIDES = 'rides'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_stations_df(df):\n",
    "    # Because someone can't make data files with uniform column names\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.columns = df.columns.str.replace('number', 'id')  # 'Station Number' vs Station ID\n",
    "    df.columns = df.columns.str.replace('date', 'time')  # 'Start Date' vs 'Start Time'\n",
    "    \n",
    "    \n",
    "    \n",
    "    df.columns = df.columns.str.replace('[\\ ]', '', regex = True)\n",
    "    # transform the dates\n",
    "    df[starttime] = df[starttime].apply(transform_date)\n",
    "    if CITY == \"boston\":\n",
    "        df = preprocess_boston_stations_df(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Some of the earlier bostons stations data does not include lat,lon coordinates.\n",
    "# These files contains the lat,lon coordinates (and other data) for station IDs\n",
    "hubway_stations_locations_filenames = [\n",
    "    \"Hubway_Stations_as_of_July_2017.csv\",\n",
    "    \"previous_Hubway_Stations_as_of_July_2017.csv\"\n",
    "]\n",
    "\n",
    "def get_hubway_stations_locations_df():\n",
    "    df = pd.DataFrame()\n",
    "    filenames = [get_filepath(CITY) + fname for fname in hubway_stations_locations_filenames]\n",
    "    for filename in filenames:\n",
    "        new_df = pd.read_csv(filename)    \n",
    "        hubway_stations_locations_column_names = {\n",
    "            \"Station ID\": start_station_id,\n",
    "            \"Latitude\": start_station_latitude,\n",
    "            \"Longitude\": start_station_longitude,\n",
    "        }\n",
    "        # Rename the column names to match the rides data that the locations data will be joined with\n",
    "        new_df.rename(columns=hubway_stations_locations_column_names, inplace=True)\n",
    "        df = new_df if df.empty else df.append(new_df)\n",
    "    df.drop_duplicates(subset=[start_station_id], inplace=True)\n",
    "    return df\n",
    "\n",
    "hubway_stations_locations_df = None\n",
    "if CITY == \"boston\":\n",
    "    hubway_stations_locations_df = get_hubway_stations_locations_df()\n",
    "\n",
    "\n",
    "def preprocess_boston_stations_df(df):\n",
    "    if start_station_latitude in df.columns:\n",
    "        return df\n",
    "    # Otherwise this is one of the datasets that is lacking lat, lon info.\n",
    "    # Add the lat,lon info\n",
    "    return hubway_stations_locations_df.merge(df, on=start_station_id)\n",
    "\n",
    "def choose_chicago_columns(filename):\n",
    "# the chicago names for files and contents of the files vary so much that a function is required\n",
    "# to get the column names\n",
    "\n",
    "    if filename == 'Divvy_Trips_2018_Q1.zip' or filename == 'Divvy_Trips_2019_Q2.zip':\n",
    "        return ('03-rentalstartstationid','03-rentalstartstationname', '', '', '01-rentaldetailslocalstarttime' )\n",
    "    if filename[:5] == 'Divvy' and filename != 'Divvy_Trips_2020_Q1.zip':\n",
    "        if filename[:7] == 'Divvy_S' or int(filename[12:16]) < 2017:\n",
    "            return ('from_station_id', 'from_station_name', '', '', 'starttime')\n",
    "        return ('from_station_id', 'from_station_name', '', '', 'start_time')\n",
    "    return ('start_station_id', 'start_station_name', 'start_lat', 'start_lng', 'started_at')\n",
    "\n",
    "\n",
    "\n",
    "# hubway_stations_locations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAMES_TO_IGNORE = hubway_stations_locations_filenames + ['indego-stations-2022-04-01.csv'] + ['stations.csv'] + ['stations.json'] + ['metro-bike-share-stations-2022-04-01.csv'] #+ ['202205-divvy-tripdata.zip']#+ ['202102-capitalbikeshare-tripdata.zip']# + [more bad filenames here]\n",
    "\n",
    "\n",
    "def stations_dict_to_df(stations_dict):\n",
    "    new_dict = {\n",
    "        ID: [],\n",
    "        NAME: [],\n",
    "        LAT: [],\n",
    "        LON: [],\n",
    "        FIRST: [],\n",
    "        LAST: [],\n",
    "        RIDES: []\n",
    "    }\n",
    "    for station_id, station_dict in stations_dict.items():\n",
    "        new_dict[ID].append(station_id)\n",
    "        new_dict[NAME].append(station_dict[NAME])\n",
    "        new_dict[LAT].append(station_dict[LAT])\n",
    "        new_dict[LON].append(station_dict[LON])\n",
    "        new_dict[FIRST].append(station_dict[FIRST])\n",
    "        new_dict[LAST].append(station_dict[LAST])\n",
    "        new_dict[RIDES].append(station_dict[RIDES])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(new_dict)\n",
    "    \n",
    "\n",
    "\n",
    "stations_dict = dict()\n",
    "needs_lat_lon = set()\n",
    "directory = get_filepath(CITY)\n",
    "files_count = 0\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    print(filename)\n",
    "    if filename in FILENAMES_TO_IGNORE:\n",
    "        continue\n",
    "    ### Logic for determineing column names, there is a lot of variation\n",
    "    if CITY == 'chicago':\n",
    "        start_station_id, start_station_name, start_station_latitude, start_station_longitude, starttime = choose_chicago_columns(filename)\n",
    "        if starttime == 'started_at':\n",
    "            # flags this file as having latitude and longitude\n",
    "            haslat = True\n",
    "        else:\n",
    "            haslat = False\n",
    "    elif CITY == 'philly':\n",
    "        \n",
    "        if ('2016' in filename or 'Q1_2017' in filename and CITY == 'philly'):\n",
    "            start_station_id = 'start_station_id'\n",
    "            start_station_name = 'start_station_id'\n",
    "        else:\n",
    "            start_station_id = 'start_station'\n",
    "            start_station_name = 'start_station' # to fill in with other data table later this is just a placeholder\n",
    "        start_station_latitude = 'start_lat'\n",
    "        start_station_longitude = 'start_lon'\n",
    "        starttime = 'start_time'\n",
    "   \n",
    "\n",
    "    elif ((filename[:4] in ['2020', '2021', '2022'] and filename[:6] not in ['202001', '202002', '202003']) and CITY == 'dc') or (filename[:4] in ['2021', '2022'] and filename[:6] != '202101' and CITY == 'nyc'):\n",
    "        start_station_id = 'start_station_id'\n",
    "        start_station_name = 'start_station_name'\n",
    "        start_station_latitude = 'start_lat'\n",
    "        start_station_longitude = 'start_lng'\n",
    "        starttime = 'started_at'\n",
    "        # flags this file as having latitude and longitude\n",
    "        haslat = True\n",
    "\n",
    "    elif CITY != 'dc':\n",
    "        start_station_id = 'startstationid'\n",
    "        start_station_name = 'startstationname'\n",
    "        start_station_latitude = 'startstationlatitude'\n",
    "        start_station_longitude = 'startstationlongitude'\n",
    "        starttime = 'starttime'\n",
    "    else:\n",
    "        start_station_id = 'startstationid'\n",
    "        start_station_name = 'startstation'\n",
    "        start_station_latitude = \"\"\n",
    "        start_station_longitude = \"\"\n",
    "        starttime = \"starttime\"\n",
    "        haslat = False\n",
    "\n",
    "\n",
    "    fullfilename = directory + filename\n",
    "    print(files_count, ': handling file', filename)\n",
    "    files_count+=1\n",
    "    \n",
    "    if filename.endswith(\".csv\"):\n",
    "        stations_dfs = [pd.read_csv(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and (CITY == 'nyc' or CITY == 'boston'):\n",
    "        stations_dfs = [open_zipfile(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY =='philly':\n",
    "        stations_dfs = [open_zipfile_philly(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY == 'dc':\n",
    "        stations_dfs = [df for df in open_zipfile_dc(fullfilename)]\n",
    "    elif filename.endswith(\".zip\") and CITY == 'chicago':\n",
    "        stations_dfs = [df for df in open_zipfile_chicago(fullfilename)]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    for stations_df in stations_dfs:\n",
    "\n",
    "        stations_df = preprocess_stations_df(stations_df)\n",
    "        \n",
    "        unique_station_ids = stations_df[start_station_id].unique()\n",
    "        for station_id in unique_station_ids:\n",
    "            station_df = stations_df[stations_df[start_station_id] == station_id]\n",
    "            ## Some cities (DC, Chicago) stated off by not putting lat and lons on ride data, therefore keep the station in a set \n",
    "            ## and if the station is used in a later year's file that has its lat and lon, update the lat and lon for the station\n",
    "            just_added = False\n",
    "            if station_id not in stations_dict:\n",
    "                try:\n",
    "                    stations_dict[station_id] = {\n",
    "                        NAME: station_df[start_station_name].iloc[0], \n",
    "                        LAT: station_df[start_station_latitude].iloc[0],\n",
    "                        LON: station_df[start_station_longitude].iloc[0], \n",
    "                        FIRST: station_df[starttime].iloc[0], \n",
    "                        LAST: station_df[starttime].iloc[0],\n",
    "                        RIDES: 0,\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    \n",
    "                    if type(e).__name__ == 'KeyError':\n",
    "                        \n",
    "                        stations_dict[station_id] = {\n",
    "                            NAME: station_df[start_station_name].iloc[0], \n",
    "                            LAT: 0,\n",
    "                            LON: 0,\n",
    "                            FIRST: station_df[starttime].iloc[0], \n",
    "                            LAST: station_df[starttime].iloc[0],\n",
    "                            RIDES: 0,\n",
    "                        }\n",
    "                        ### Set of stations that need a latitude and longitude\n",
    "                        needs_lat_lon.add(station_id)\n",
    "                        just_added = True\n",
    "                    else:    \n",
    "                        continue\n",
    "            # if the station needs lat and lon and the station has a lat and lon, give it to the station dict\n",
    "            if station_id in needs_lat_lon and haslat:\n",
    "                stations_dict[station_id][LAT] = station_df[start_station_latitude].iloc[0]\n",
    "                stations_dict[station_id][LON] = station_df[start_station_longitude].iloc[0]\n",
    "                needs_lat_lon.remove(station_id)\n",
    "            rides_count = len(station_df.index)\n",
    "            stations_dict[station_id][RIDES] += rides_count\n",
    "            station_df = station_df.sort_values(by=[starttime])\n",
    "            if (station_df[starttime].iloc[0] < stations_dict[station_id][FIRST]):\n",
    "                stations_dict[station_id][FIRST] = stations_df[starttime].iloc[0]\n",
    "            if (station_df[starttime].iloc[-1] > stations_dict[station_id][LAST]):\n",
    "                stations_dict[station_id][LAST] = stations_df[starttime].iloc[-1]\n",
    "\n",
    "## Philadelphia trips dont come with station names, that comes from a file with all the station names\n",
    "## so the names are added afterwards to the data frame witht his file\n",
    "if CITY == 'philly':\n",
    "    name_df = pd.read_csv(get_filepath(CITY) + '/indego-stations-2022-04-01.csv')\n",
    "    for index, row in name_df.iterrows():\n",
    "        if row['Station_ID'] in stations_dict:\n",
    "            stations_dict[row['Station_ID']][NAME] = row['Station_Name']\n",
    "\n",
    "\n",
    "stations_df = stations_dict_to_df(stations_dict)\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For the boston hubway/blue bikes data there will be duplicates because\n",
    "when management changed from hubway to Bluebikes, the data fromat did too\n",
    "This includes the station id/numbers a\n",
    "nd names AND lat/lon!\n",
    "Task: deduplicate stations\n",
    "\n",
    "Idea to understand data: sort the stations so the potential duplicates are next to each other\n",
    "when merging/deduping data make sure to keep the earliest first and the latest last.\n",
    "\n",
    "approach to deduplicating stations:\n",
    "- normalize names and add new temporary column with normalized name\n",
    "- get list of unique normalized names\n",
    "- for each name:\n",
    "    make a df for that name, sorted by [first, last]\n",
    "    update main df to replace entries with that name with:\n",
    "        first first\n",
    "        last last\n",
    "        last name\n",
    "        rides as sum of rides\n",
    "    sort main df by [name, first] and drop duplicates (duplicates on normalized name)\n",
    "    remove normalized name column\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "pd.options.mode.chained_assignment = None\n",
    "NORMALIZED_NAME = 'normalized_name'\n",
    "\n",
    "def normalized_station_name(name):\n",
    "    normalized_name = name.lower()\n",
    "    normalized_name = normalized_name.replace(\"former\",  \"\").replace(\" \", \"\")\n",
    "    # combines stations that have temp in them if a newer stations has been made\n",
    "    # this happens in chicago\n",
    "    if '(temp)' in normalized_name:\n",
    "        normalized_name = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", normalized_name)\n",
    "    normalized_name = re.sub(r'[^a-z0-9]','', normalized_name)\n",
    "    return normalized_name\n",
    "\n",
    "if CITY == 'boston' or CITY == 'chicago' or CITY == 'dc' or CITY == 'nyc':\n",
    "    stations_df[NORMALIZED_NAME] = stations_df[NAME].apply(normalized_station_name)\n",
    "    normalized_names = stations_df[NORMALIZED_NAME]\n",
    "    print(normalized_names.shape[0], ' names')\n",
    "    unique_normalized_names = stations_df[NORMALIZED_NAME].unique()\n",
    "    print(unique_normalized_names.shape[0], ' unique normalized names') #, unique_normalized_names)\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    for normalized_name in unique_normalized_names:\n",
    "        print(n, 'handling name', normalized_name)\n",
    "        n+=1\n",
    "        name_df = stations_df[stations_df[NORMALIZED_NAME] == normalized_name]\n",
    "        name_df.sort_values(by=[FIRST, LAST], inplace=True)\n",
    "        first = name_df[FIRST].iloc[0]\n",
    "        last = name_df[LAST].iloc[-1]\n",
    "        name = name_df[NAME].iloc[-1]\n",
    "        rides = name_df[RIDES].sum()\n",
    "        update_condition = (stations_df[NORMALIZED_NAME] == normalized_name)\n",
    "        stations_df.loc[update_condition, [FIRST, LAST, NAME, RIDES]] = first, last, name, rides\n",
    "\n",
    "    stations_dropped_duplicates_df = stations_df.drop_duplicates(subset=[NORMALIZED_NAME])\n",
    "    print('dropped %s rows based on duplicate names' % (int(stations_df.shape[0]) - int(stations_dropped_duplicates_df.shape[0])))\n",
    "    stations_dropped_duplicates_df.drop(labels=[NORMALIZED_NAME], axis=1, inplace=True)\n",
    "    \n",
    "    stations_df = stations_dropped_duplicates_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the stations_df\n",
    "# Remove dummy stations (there are test stations in the data)\n",
    "# Remove stations with less than RIDES_COUNT_THRESHOLD rides\n",
    "bad_stations_df = stations_df[stations_df[RIDES] < RIDES_COUNT_THRESHOLD]\n",
    "print('removing %d bad stations that each have less than %d rides from stations data' % (bad_stations_df.shape[0], RIDES_COUNT_THRESHOLD))\n",
    "stations_df = stations_df[stations_df[RIDES] >= RIDES_COUNT_THRESHOLD]\n",
    "## Remove stations that do not have a latitude and longitude measure\n",
    "bad_stations_2 = stations_df[stations_df[LAT] == 0]\n",
    "print(f'removing {bad_stations_2.shape[0]} more stations for not having latitude or longitude')\n",
    "stations_df = stations_df[stations_df[LAT] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_stations_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mainly for DC, some stations are formatted incorrectly that are copies of other stations only for one month, these stations are removed\n",
    "\"\"\"\n",
    "initial_length = len(stations_df.name)\n",
    "stations_df.drop(stations_df[(stations_df['first'] >stations_df['last']) ].index, inplace=True)\n",
    "print(f' removed {initial_length - len(stations_df.name)} stations for having bad dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to CSV\n",
    "save_to_csvfilename = directory + 'stations.csv'\n",
    "stations_df.to_csv(save_to_csvfilename)\n",
    "\n",
    "stations_df = pd.read_csv(save_to_csvfilename)\n",
    "initial = len(stations_df.name)\n",
    "searchfor = ['test', 'TEST', 'virtual', 'Virtual']\n",
    "bad_df = stations_df[stations_df.name.str.contains('|'.join(searchfor))]\n",
    "stations_df = stations_df[~stations_df.name.str.contains('|'.join(searchfor))]\n",
    "for val in stations_df.id:\n",
    "    if isinstance(val, str):\n",
    "        bad_df_2 = stations_df.loc[~((stations_df.id.str.isalnum()) | stations_df.id.str.contains('.0') | stations_df.id.str.contains('.1')| stations_df.id.str.contains('.2') | stations_df.id.str.contains('.3') | stations_df.id.str.contains('.4') | stations_df.id.str.contains('.5') | stations_df.id.str.contains('.6') | stations_df.id.str.contains('.7')| stations_df.id.str.contains('.8')| stations_df.id.str.contains('.9'))]\n",
    "    \n",
    "        stations_df = stations_df.loc[((stations_df.id.str.isalnum()) | stations_df.id.str.contains('.0') | stations_df.id.str.contains('.1')| stations_df.id.str.contains('.2') | stations_df.id.str.contains('.3') | stations_df.id.str.contains('.4') | stations_df.id.str.contains('.5') | stations_df.id.str.contains('.6') | stations_df.id.str.contains('.7')| stations_df.id.str.contains('.8')| stations_df.id.str.contains('.9'))]\n",
    "    break\n",
    "print(f'{initial - len(stations_df.name)} stations removed for bad names')\n",
    "stations_df.to_csv(save_to_csvfilename)\n",
    "\n",
    "\n",
    "print('wrote data to ', save_to_csvfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to JSON that will be used in web app\n",
    "import json\n",
    "\n",
    "stations = []\n",
    "for index, row in stations_df.iterrows():\n",
    "    # Transform the date\n",
    "    date = row[5]\n",
    "    \n",
    "    stations.append({\n",
    "        ID: str(row[ID]),\n",
    "        NAME: str(row[NAME]),\n",
    "        LAT: row[LAT],\n",
    "        LON: row[LON],\n",
    "        FIRST: transform_date(row[FIRST]),\n",
    "        LAST: transform_date(row[LAST]),\n",
    "    })\n",
    "\n",
    "json = json.dumps(stations)\n",
    "\n",
    "save_to_jsonfilename = directory + 'stations.json'\n",
    "with open(save_to_jsonfilename, 'w') as f:\n",
    "    f.write(json)\n",
    "print(\"Data written to stations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1bbe9456d20300904e9b13151849e2cec71d81e4372ca48313873a0c8e6e415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
